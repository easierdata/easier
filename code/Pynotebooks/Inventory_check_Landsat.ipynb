{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory and Download Landsat from USGS\n",
    "Workaround Solution: Use brower-cookie3 to \"sign\" the URL request so a python script can stream data from USGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install browser-cookie3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import browser_cookie3\n",
    "import requests\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "from pystac.item import Item\n",
    "import pandas as pd\n",
    "from dateutil.rrule import rrule, DAILY\n",
    "import datetime\n",
    "from datetime import date\n",
    "import glob\n",
    "import tqdm\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect inventory from USGS\n",
    "Query guide: https://landsatlook.usgs.gov/stac-server/api.html#tag/Item-Search\n",
    "\n",
    "\n",
    "Example URL:\n",
    "https://landsatlook.usgs.gov/stac-server/collections/landsat-c2l1/items?limit=10000&datetime=2022-01-06T00:00:00Z/2022-01-07T00:00:00Z&fields=id,-type,-geometry,-bbox,-properties,-links,-assets,-collection,-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cltns = [\"landsat-c2l2-sr\",\t# Landsat Collection 2 Level-2 UTM Surface Reflectance (SR) Product\n",
    "\"landsat-c2l2-st\",\t# Landsat Collection 2 Level-2 UTM Surface Temperature (ST) Product\n",
    "\"landsat-c2ard-st\",\t# Landsat Collection 2 Analysis Ready Data (ARD) Level-2 UTM Surface Temperature (ST) Product\n",
    "\"landsat-c2l2alb-bt\",\t# Landsat Collection 2 Level-2 Albers Top of Atmosphere Brightness Temperature (BT) Product\n",
    "\"landsat-c2l3-fsca\",\t# Landsat Collection 2 Level-3 Fractional Snow Covered Area (fSCA) Product\n",
    "\"landsat-c2ard-bt\",\t# Landsat Collection 2 Analysis Ready Data (ARD) Level-2 UTM Top of Atmosphere Brightness Temperature (BT) Product\n",
    "\"landsat-c2l1\",\t# Landsat Collection 2 Level-1 Product\n",
    "\"landsat-c2l3-ba\",\t# Landsat Collection 2 Level-3 Burned Area (BA) Product\n",
    "\"landsat-c2l2alb-st\",\t# Landsat Collection 2 Level-2 Albers Surface Temperature (ST) Product\n",
    "\"landsat-c2ard-sr\",\t# Landsat Collection 2 Analysis Ready Data (ARD) Level-2 UTM Surface Reflectance (SR) Product\n",
    "\"landsat-c2l2alb-sr\",\t# Landsat Collection 2 Level-2 Albers Surface Reflectance (SR) Product\n",
    "\"landsat-c2l2alb-ta\",\t# Landsat Collection 2 Level-2 Albers Top of Atmosphere (TA) Reflectance Product\n",
    "\"landsat-c2l3-dswe\",\t #Landsat Collection 2 Level-3 Dynamic Surface Water Extent (DSWE) Product\n",
    "\"landsat-c2ard-ta\"]    #\tLandsat Collection 2 Analysis Ready Data (ARD) Level-2 UTM Top of Atmosphere (TA) Reflectance Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for cltn in cltns:\n",
    "    directory = f\"../../data/landsat/stac/{cltn}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cltn = \"landsat-c2l1\"\n",
    "\n",
    "yymmdd = \"2022-10-22\"\n",
    "\n",
    "url = f'''https://landsatlook.usgs.gov/stac-server/collections/{cltn}/items?limit=10000&datetime={yymmdd}T00:00:00Z/{yymmdd}T23:59:59Z&fields=id,-type,-geometry,-bbox,properties,-links,-assets,-collection,-features'''\n",
    "\n",
    "with urlopen(url) as uo:\n",
    "    data = json.load(uo)\n",
    "    #print(data)\n",
    "    feature_df = pd.json_normalize(data['features'])\n",
    "\n",
    "l9 = feature_df.query('`properties.platform`==\"LANDSAT_9\"')\n",
    "\n",
    "l9[l9.id.str.startswith(\"LT09_L1GT_137\")]\n",
    "stat_list = []\n",
    "\n",
    "with urlopen(url) as uo:\n",
    "    data = json.load(uo)\n",
    "    #print(data)\n",
    "    feature_df = pd.json_normalize(data['features'])\n",
    "    l9 = feature_df.query('`properties.platform`==\"LANDSAT_9\"')\n",
    "    l8 = feature_df.query('`properties.platform`==\"LANDSAT_8\"')\n",
    "    stat_list.append({'date':yymmdd,'total':data['numberMatched'],'return':data['numberReturned'],'LC09':len(l9),'LC08':len(l9)})\n",
    "    feature_df.drop('type',axis=1).to_csv(f'../../data/landsat/stac/{cltn}_{dt.strftime(\"%Y-%m-%d\")}.csv',index=False)\n",
    "\n",
    "\n",
    "pd.DataFrame(stat_list).query('`date` > \"2021-09-27\"').to_csv('../../data/landsat/day_sum_LC09_LC08.csv',index=False)\n",
    "day_sum = pd.DataFrame(stat_list).query('`date` > \"2021-09-27\"')\n",
    "\n",
    "pd.json_normalize(data['features']).query('`properties.platform`==\"LANDSAT_9\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cltn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inventory_collection(cltn):\n",
    "    a = date(2022, 5, 1)\n",
    "    b = date(2022, 11, 1)\n",
    "    stat_list=[]\n",
    "    for dt in rrule(DAILY, dtstart=a, until=b):\n",
    "        #print(dt.strftime(\"%Y-%m-%d\"))\n",
    "        yymmdd = dt.strftime(\"%Y-%m-%d\")\n",
    "        url = f'''https://landsatlook.usgs.gov/stac-server/collections/{cltn}/items?limit=10000&datetime={yymmdd}T00:00:00Z/{yymmdd}T23:59:59Z&fields=id,-type,-geometry,-bbox,properties,-links,-assets,-collection,-features'''\n",
    "        try:\n",
    "            with urlopen(url) as uo:\n",
    "                data = json.load(uo)\n",
    "                #print(data)\n",
    "                if not data['features']:\n",
    "                    continue\n",
    "                feature_df = pd.json_normalize(data['features'])\n",
    "                l9 = feature_df.query('`properties.platform`==\"LANDSAT_9\"')\n",
    "                l8 = feature_df.query('`properties.platform`==\"LANDSAT_8\"')\n",
    "                stat_list.append({'date':yymmdd,'total':data['numberMatched'],'return':data['numberReturned'],'LC09':len(l9),'LC08':len(l8)})\n",
    "                if len(l9)>0:\n",
    "                    l9.drop('type',axis=1).to_csv(f'../../data/stac/{cltn}/csv/{cltn}_{dt.strftime(\"%Y-%m-%d\")}_L9.csv',index=False)\n",
    "                #print(stat_list[-1])\n",
    "        except e:\n",
    "            print(e)\n",
    "    pd.DataFrame(stat_list).to_csv(f'../../data/stac/stat/{cltn}_day_sum_LC09_LC08.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_collection(\"landsat-c2l1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cltn in cltns[4:-1]:\n",
    "    print(cltn)\n",
    "    inventory_collection(cltn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cltn in cltns:\n",
    "    print(cltn,pd.read_csv(f'../../data/landsat/{cltn}_day_sum_LC09_LC08.csv').LC09.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check progress and missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_m2m = [fp.split('/')[-1][:-8] for fp in glob.glob(\"../../data/landsat/m2m_download/*/*/*B11.TIF\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_m2m = [fp.split('/')[-1][:-10] for fp in glob.glob(\"../../data/landsat/m2m_download/*/*/*_stac.json\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_jpg = [fp.split('/')[-1][:-11] for fp in glob.glob(\"../../data/landsat/m2m_download/*/*/*_large.jpeg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_stac = list(set(cached_m2m)-set(complete_m2m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_m2m= [fp.split('/')[-1][:-10] for fp in glob.glob(\"../../data/stac/landsat-c2l1/json/*/*.json\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare differences and generate task lists\n",
    "First run `find . -type f -name '*.tar' -exec basename {} .tar \\; > landsat_avail.txt` on local inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_scenes_list(many_scenes,batch_tag,batch_size=100):\n",
    "    batch_num = len(many_scenes) // batch_size +1\n",
    "    if len(many_scenes) % batch_size == 0:\n",
    "        batch_num-=1\n",
    "    import os\n",
    "    dest = f'../../data/landsat/task/{batch_tag}'\n",
    "    try:\n",
    "        os.makedirs(dest)\n",
    "    except FileExistsError:\n",
    "       # directory already exists\n",
    "       pass\n",
    "    for i in range(1,batch_num+1):\n",
    "        with open(f'{dest}/scenes_{str(i).zfill(4)}.txt', 'w') as fp:\n",
    "            fp.write(\"landsat_ot_c2_l1|displayId\\n\")\n",
    "            for item in many_scenes[(i-1)*batch_size:i*batch_size]:\n",
    "                # write each item on a new line\n",
    "                fp.write(f\"{item}\\n\")\n",
    "    print('Done',batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_inventory = pd.concat([pd.read_csv(f) for f in glob.glob('../../data/stac/landsat-c2l1/csv/*.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_finished = set(finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_inventory['aq_time'] = pd.to_datetime(usgs_inventory['properties.datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_first_year = usgs_inventory[usgs_inventory['aq_time']<\"2022-11-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_first_year[usgs_first_year.duplicated(\"properties.landsat:scene_id\",False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(usgs_first_year[\"properties.landsat:scene_id\"].unique()) - set(usgs_first_year[usgs_first_year.id.isin(finished_m2m)][\"properties.landsat:scene_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_first_year[usgs_first_year[\"properties.landsat:scene_id\"].isin({'LT91372062022295LGN00', 'LT91372072022295LGN00', 'LT91372082022295LGN00'})].id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_list = list(set(usgs_first_year.id.to_list())-set(finished_m2m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(export_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_list = usgs_first_year[usgs_first_year[\"properties.landsat:scene_id\"].isin({'LT91372062022295LGN00', 'LT91372072022295LGN00', 'LT91372082022295LGN00'})].id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2m_batch2 = usgs_inventory[usgs_inventory['aq_time']>=\"2022-05-01\"].sort_values('aq_time').id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_scenes_list(export_list,'batch_ms1',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2m_batch2 = usgs_inventory[(\"2022-05-01\" <= usgs_inventory['aq_time'] )&(usgs_inventory['aq_time']<= \"2022-05-31\")].sort_values('aq_time').id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2m_batch2 = usgs_inventory[(\"2021-11-30\" < usgs_inventory['aq_time'] )&(usgs_inventory['aq_time']< \"2022-01-01\")].sort_values('aq_time').id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch3 = usgs_inventory[(\"2022-01-01\" <= usgs_inventory['aq_time'] )&(usgs_inventory['aq_time']< \"2022-02-01\")].sort_values('aq_time').id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(batch3)-set(finished_m2m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_scenes_list(list(set(batch3)-set(finished_m2m)),'batch3',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2022\n",
    "month = 10\n",
    "date1 = f\"{year}-{str(month).zfill(2)}-01\"\n",
    "date1fix = f\"{year}-{str(month).zfill(2)}-{calendar.monthrange(year, month)[1]}\"\n",
    "dt2 = datetime.datetime.strptime(date1, \"%Y-%m-%d\")+datetime.timedelta(days=calendar.monthrange(year, month)[1])\n",
    "date2 = datetime.datetime.strftime(dt2,\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = \"2022-10-01\"\n",
    "date2 = \"2022-11-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_auto = usgs_inventory[(date1 < usgs_inventory['aq_time'] )&(usgs_inventory['aq_time']<= date2)].sort_values('aq_time').id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(batch_auto) - set(finished_m2m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missings = list(set(batch_auto) - set(finished_m2m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_scenes_list(missings,f'batch_name',200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Official USGS script for scene download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#official script for m2m api\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import re\n",
    "import threading\n",
    "import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "path = \"../../data/landsat/m2m_download\" # Fill a valid download path\n",
    "maxthreads = 10 # Threads count for downloads\n",
    "sema = threading.Semaphore(value=maxthreads)\n",
    "label = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") # Customized label using date time\n",
    "threads = []\n",
    "# The entityIds/displayIds need to save to a text file such as scenes.txt.\n",
    "# The header of text file should follow the format: datasetName|displayId or datasetName|entityId. \n",
    "# sample file - scenes.txt\n",
    "# landsat_ot_c2_l2|displayId\n",
    "# LC08_L2SP_012025_20201231_20210308_02_T1\n",
    "# LC08_L2SP_012027_20201215_20210314_02_T1\n",
    "scenesFile = '../pyScripts/scenes2.txt'\n",
    "\n",
    "# Send http request\n",
    "def sendRequest(url, data, apiKey = None, exitIfNoResponse = True):  \n",
    "    json_data = json.dumps(data)\n",
    "    \n",
    "    if apiKey == None:\n",
    "        response = requests.post(url, json_data)\n",
    "    else:\n",
    "        headers = {'X-Auth-Token': apiKey}              \n",
    "        response = requests.post(url, json_data, headers = headers)  \n",
    "    \n",
    "    try:\n",
    "      httpStatusCode = response.status_code \n",
    "      if response == None:\n",
    "          print(\"No output from service\")\n",
    "          if exitIfNoResponse: sys.exit()\n",
    "          else: return False\n",
    "      output = json.loads(response.text)\n",
    "      if output['errorCode'] != None:\n",
    "          print(output['errorCode'], \"- \", output['errorMessage'])\n",
    "          if exitIfNoResponse: sys.exit()\n",
    "          else: return False\n",
    "      if  httpStatusCode == 404:\n",
    "          print(\"404 Not Found\")\n",
    "          if exitIfNoResponse: sys.exit()\n",
    "          else: return False\n",
    "      elif httpStatusCode == 401: \n",
    "          print(\"401 Unauthorized\")\n",
    "          if exitIfNoResponse: sys.exit()\n",
    "          else: return False\n",
    "      elif httpStatusCode == 400:\n",
    "          print(\"Error Code\", httpStatusCode)\n",
    "          if exitIfNoResponse: sys.exit()\n",
    "          else: return False\n",
    "    except Exception as e: \n",
    "          response.close()\n",
    "          print(e)\n",
    "          if exitIfNoResponse: sys.exit()\n",
    "          else: return False\n",
    "    response.close()\n",
    "    \n",
    "    return output['data']\n",
    "\n",
    "def downloadFile(url):\n",
    "    sema.acquire()\n",
    "    try:        \n",
    "        response = requests.get(url, stream=True)\n",
    "        disposition = response.headers['content-disposition']\n",
    "        filename = re.findall(\"filename=(.+)\", disposition)[0].strip(\"\\\"\")\n",
    "        print(f\"Downloading {filename} ...\\n\")\n",
    "        if path != \"\" and path[-1] != \"/\":\n",
    "            filename = \"/\" + filename\n",
    "        open(path+filename, 'wb').write(response.content)\n",
    "        print(f\"Downloaded {filename}\\n\")\n",
    "        sema.release()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download from {url}. Will try to re-download.\")\n",
    "        sema.release()\n",
    "        runDownload(threads, url)\n",
    "    \n",
    "def runDownload(threads, url):\n",
    "    thread = threading.Thread(target=downloadFile, args=(url,))\n",
    "    threads.append(thread)\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv(\"M2M_USER\")\n",
    "password = os.getenv(\"M2M_PSWD\")  \n",
    "\n",
    "print(\"\\nRunning Scripts...\\n\")\n",
    "\n",
    "serviceUrl = \"https://m2m.cr.usgs.gov/api/api/json/stable/\"\n",
    "\n",
    "# login\n",
    "payload = {'username' : username, 'password' : password}\n",
    "\n",
    "apiKey = sendRequest(serviceUrl + \"login\", payload)\n",
    "\n",
    "print(\"API Key: \" + apiKey + \"\\n\")\n",
    "\n",
    "datasetName = \"landsat_ot_c2_l1\"\n",
    "\n",
    "spatialFilter =  {'filterType' : \"mbr\",\n",
    "                  'lowerLeft' : {'latitude' : 30, 'longitude' : -120},\n",
    "                  'upperRight' : { 'latitude' : 40, 'longitude' : -140}}\n",
    "\n",
    "temporalFilter = {'start' : '2021-10-10', 'end' : '2025-12-10'}\n",
    "\n",
    "payload = {'datasetName' : datasetName,\n",
    "                           'temporalFilter' : temporalFilter}                     \n",
    "\n",
    "print(\"Searching datasets...\\n\")\n",
    "datasets = sendRequest(serviceUrl + \"dataset-search\", payload, apiKey)\n",
    "\n",
    "print(\"Found \", len(datasets), \" datasets\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisitionFilter = {\"end\": \"2021-11-30\",\n",
    "                             \"start\": \"2021-10-10\" }        \n",
    "metadataValue = {\"filterType\":\"value\",\n",
    "                  \"filterId\":\"61af93b8fad2acf5\",\n",
    "                  \"value\":\"9\",\n",
    "                   \"operand\":\"=\"\n",
    "                 }\n",
    "\n",
    "\n",
    "payload = {'datasetName' : 'landsat_ot_c2_l1', \n",
    "                         'maxResults' : 50000,\n",
    "                         #'startingNumber' : 1, \n",
    "                         'sceneFilter' : {'metadataFilter':metadataValue,\n",
    "                                          #'spatialFilter' : spatialFilter,\n",
    "                                          'acquisitionFilter' : acquisitionFilter}}\n",
    "\n",
    "# Now I need to run a scene search to find data to download\n",
    "print(\"Searching scenes...\\n\\n\")   \n",
    "\n",
    "scenes = sendRequest(serviceUrl + \"scene-search\", payload, apiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv(\"M2M_USER\")\n",
    "password = os.getenv(\"M2M_PSWD\")\n",
    "filetype = 'bundle'\n",
    "\n",
    "print(\"\\nRunning Scripts...\\n\")\n",
    "startTime = time.time()\n",
    "\n",
    "serviceUrl = \"https://m2m.cr.usgs.gov/api/api/json/stable/\"\n",
    "\n",
    "# Login\n",
    "payload = {'username' : username, 'password' : password}    \n",
    "apiKey = sendRequest(serviceUrl + \"login\", payload)    \n",
    "print(\"API Key: \" + apiKey + \"\\n\")\n",
    "\n",
    "# Read scenes\n",
    "f = open(scenesFile, \"r\")\n",
    "lines = f.readlines()   \n",
    "f.close()\n",
    "header = lines[0].strip()\n",
    "datasetName = header[:header.find(\"|\")]\n",
    "idField = header[header.find(\"|\")+1:]\n",
    "\n",
    "print(\"Scenes details:\")\n",
    "print(f\"Dataset name: {datasetName}\")\n",
    "print(f\"Id field: {idField}\\n\")\n",
    "\n",
    "entityIds = []\n",
    "\n",
    "lines.pop(0)\n",
    "for line in lines:        \n",
    "    entityIds.append(line.strip())\n",
    "\n",
    "# Search scenes \n",
    "# If you don't have a scenes text file that you can use scene-search to identify scenes you're interested in\n",
    "# https://m2m.cr.usgs.gov/api/docs/reference/#scene-search\n",
    "# payload = { \n",
    "#             'datasetName' : '', # dataset alias\n",
    "#             'maxResults' : 10, # max results to return\n",
    "#             'startingNumber' : 1, \n",
    "#             'sceneFilter' : {} # scene filter\n",
    "#           }\n",
    "\n",
    "# results = sendRequest(serviceUrl + \"scene-search\", payload, apiKey)  \n",
    "# for result in results:\n",
    "#     entityIds.append(result['entityId'])\n",
    "\n",
    "# Add scenes to a list\n",
    "listId = f\"temp_{datasetName}_list\" # customized list id\n",
    "payload = {\n",
    "    \"listId\": listId,\n",
    "    'idField' : idField,\n",
    "    \"entityIds\": entityIds,\n",
    "    \"datasetName\": datasetName\n",
    "}\n",
    "\n",
    "print(\"Adding scenes to list...\\n\")\n",
    "count = sendRequest(serviceUrl + \"scene-list-add\", payload, apiKey)    \n",
    "print(\"Added\", count, \"scenes\\n\")\n",
    "\n",
    "# Get download options\n",
    "payload = {\n",
    "    \"listId\": listId,\n",
    "    \"datasetName\": datasetName\n",
    "}\n",
    "\n",
    "print(\"Getting product download options...\\n\")\n",
    "products = sendRequest(serviceUrl + \"download-options\", payload, apiKey)\n",
    "print(\"Got product download options\\n\")\n",
    "\n",
    "# Select products\n",
    "downloads = []\n",
    "if filetype == 'bundle':\n",
    "    # select bundle files\n",
    "    for product in products:        \n",
    "        if product[\"bulkAvailable\"]:               \n",
    "            downloads.append({\"entityId\":product[\"entityId\"], \"productId\":product[\"id\"]})\n",
    "elif filetype == 'band':\n",
    "    # select band files\n",
    "    for product in products:  \n",
    "        if product[\"secondaryDownloads\"] is not None and len(product[\"secondaryDownloads\"]) > 0:\n",
    "            for secondaryDownload in product[\"secondaryDownloads\"]:\n",
    "                if secondaryDownload[\"bulkAvailable\"]:\n",
    "                    downloads.append({\"entityId\":secondaryDownload[\"entityId\"], \"productId\":secondaryDownload[\"id\"]})\n",
    "else:\n",
    "    # select all available files\n",
    "    for product in products:        \n",
    "        if product[\"bulkAvailable\"]:               \n",
    "            downloads.append({\"entityId\":product[\"entityId\"], \"productId\":product[\"id\"]})\n",
    "            if product[\"secondaryDownloads\"] is not None and len(product[\"secondaryDownloads\"]) > 0:\n",
    "                for secondaryDownload in product[\"secondaryDownloads\"]:\n",
    "                    if secondaryDownload[\"bulkAvailable\"]:\n",
    "                        downloads.append({\"entityId\":secondaryDownload[\"entityId\"], \"productId\":secondaryDownload[\"id\"]})\n",
    "\n",
    "# Remove the list\n",
    "payload = {\n",
    "    \"listId\": listId\n",
    "}\n",
    "sendRequest(serviceUrl + \"scene-list-remove\", payload, apiKey)                \n",
    "\n",
    "# Send download-request\n",
    "payLoad = {\n",
    "    \"downloads\": downloads,\n",
    "    \"label\": label,\n",
    "    'returnAvailable': True\n",
    "}\n",
    "\n",
    "print(f\"Sending download request ...\\n\")\n",
    "results = sendRequest(serviceUrl + \"download-request\", payLoad, apiKey)\n",
    "print(f\"Done sending download request\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results['availableDownloads']:\n",
    "    u = urlparse(result['url'])\n",
    "    if u.path.split('/')[-1]!='gen-browse':\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results['availableDownloads']:       \n",
    "    print(f\"Get download url: {result['url']}\\n\" )\n",
    "    runDownload(threads, result['url'])\n",
    "\n",
    "preparingDownloadCount = len(results['preparingDownloads'])\n",
    "preparingDownloadIds = []\n",
    "if preparingDownloadCount > 0:\n",
    "    for result in results['preparingDownloads']:  \n",
    "        preparingDownloadIds.append(result['downloadId'])\n",
    "\n",
    "    payload = {\"label\" : label}                \n",
    "    # Retrieve download urls\n",
    "    print(\"Retrieving download urls...\\n\")\n",
    "    results = sendRequest(serviceUrl + \"download-retrieve\", payload, apiKey, False)\n",
    "    if results != False:\n",
    "        for result in results['available']:\n",
    "            if result['downloadId'] in preparingDownloadIds:\n",
    "                preparingDownloadIds.remove(result['downloadId'])\n",
    "                print(f\"Get download url: {result['url']}\\n\" )\n",
    "                runDownload(threads, result['url'])\n",
    "\n",
    "        for result in results['requested']:   \n",
    "            if result['downloadId'] in preparingDownloadIds:\n",
    "                preparingDownloadIds.remove(result['downloadId'])\n",
    "                print(f\"Get download url: {result['url']}\\n\" )\n",
    "                runDownload(threads, result['url'])\n",
    "\n",
    "    # Don't get all download urls, retrieve again after 30 seconds\n",
    "    while len(preparingDownloadIds) > 0: \n",
    "        print(f\"{len(preparingDownloadIds)} downloads are not available yet. Waiting for 30s to retrieve again\\n\")\n",
    "        time.sleep(30)\n",
    "        results = sendRequest(serviceUrl + \"download-retrieve\", payload, apiKey, False)\n",
    "        if results != False:\n",
    "            for result in results['available']:                            \n",
    "                if result['downloadId'] in preparingDownloadIds:\n",
    "                    preparingDownloadIds.remove(result['downloadId'])\n",
    "                    print(f\"Get download url: {result['url']}\\n\" )\n",
    "                    runDownload(threads, result['url'])\n",
    "\n",
    "print(\"\\nGot download urls for all downloads\\n\")                \n",
    "# Logout\n",
    "endpoint = \"logout\"  \n",
    "if sendRequest(serviceUrl + endpoint, None, apiKey) == None:        \n",
    "    print(\"Logged Out\\n\")\n",
    "else:\n",
    "    print(\"Logout Failed\\n\")  \n",
    "\n",
    "print(\"Downloading files... Please do not close the program\\n\")\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "print(\"Complete Downloading\")\n",
    "\n",
    "executionTime = round((time.time() - startTime), 2)\n",
    "print(f'Total time: {executionTime} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Download Workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background step: login in USGS EROS so the brower cookie can skip the redirect of USGS when request\n",
    "\n",
    "cj = browser_cookie3.firefox(domain_name='usgs.gov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_asset(url,path):\n",
    "    r = requests.get(url, stream = True,cookies=cj)\n",
    "    if r.status_code == 200:\n",
    "        # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "        r.raw.decode_content = True\n",
    "        \n",
    "        # Open a local file with wb ( write binary ) permission.\n",
    "        with open(path,'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_item_url = f\"https://landsatlook.usgs.gov/stac-server/collections/landsat-c2l1/items/LC09_L1TP_094083_20211110_20220119_02_T1\"\n",
    "#read stac\n",
    "r = urlopen(usgs_item_url).read()\n",
    "stac_json = json.loads(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hand_pull_product(product_id,collection=\"landsat-c2l1\",tarfile=True,verbose=False):\n",
    "    \"\"\"\n",
    "    product_id: e.g.\"LC09_L1TP_086075_20220509_20220510_02_T1\"\n",
    "    \"\"\"\n",
    "    directory = f\"../../data/landsat/hand_pull/{product_id}\"\n",
    "    if os.path.exists(directory+\".tar\"):\n",
    "        #print('skip',product_id)\n",
    "        return\n",
    "    usgs_item_url = f\"https://landsatlook.usgs.gov/stac-server/collections/{collection}/items/{product_id}\"\n",
    "    #aws_item_url = \"https://jcz3phgts3.execute-api.us-west-2.amazonaws.com/dev/getobject/collection02/level-1/standard/oli-tirs/2021/094/083/LC09_L1TP_094083_20211110_20220119_02_T1/LC09_L1TP_094083_20211110_20220119_02_T1_stac.json\"\n",
    "    #read stac\n",
    "    r = urlopen(usgs_item_url).read()\n",
    "    stac_json = json.loads(r)\n",
    "    scene = Item.from_dict(stac_json)\n",
    "    assets_urls = [[k,scene.assets[k].href] for k in scene.get_assets().keys() if k != 'index']\n",
    "    directory = f\"../../data/landsat/hand_pull/{scene.id}\" #in case there is a miss match\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    #loop run the assets\n",
    "    for k,url in assets_urls:\n",
    "        path = f\"{directory}/{Path(url).name}\"\n",
    "        #print(k,path)\n",
    "        if verbose:\n",
    "            print(url)\n",
    "        cache_asset(url,path)\n",
    "    # save stac information\n",
    "    with open(f\"{directory}/{scene.id}_stac.json\",\"w\") as w:\n",
    "        json.dump(stac_json,w)\n",
    "    if tarfile:\n",
    "        output = shutil.make_archive(directory, 'tar', Path(directory).parent, Path(directory).name)\n",
    "        print(output)\n",
    "        shutil.rmtree(directory)\n",
    "    else:\n",
    "        print(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hand_pull_files(product_id,collection=\"landsat-c2l1\",verbose=False):\n",
    "    \"\"\"\n",
    "    product_id: e.g.\"LC09_L1TP_086075_20220509_20220510_02_T1\"\n",
    "    \"\"\"\n",
    "    dt = product_id.split('_')[3]\n",
    "    directory = f\"../../data/landsat/m2m_download/{dt}/{product_id}\"\n",
    "    \n",
    "    # if os.path.exists(directory+f\"/{product_id}_stac.json\"):\n",
    "    #     print('skip',product_id)\n",
    "    #     return\n",
    "    usgs_item_url = f\"https://landsatlook.usgs.gov/stac-server/collections/{collection}/items/{product_id}\"\n",
    "    #aws_item_url = \"https://jcz3phgts3.execute-api.us-west-2.amazonaws.com/dev/getobject/collection02/level-1/standard/oli-tirs/2021/094/083/LC09_L1TP_094083_20211110_20220119_02_T1/LC09_L1TP_094083_20211110_20220119_02_T1_stac.json\"\n",
    "    #read stac\n",
    "    r = urlopen(usgs_item_url).read()\n",
    "    stac_json = json.loads(r)\n",
    "    scene = Item.from_dict(stac_json)\n",
    "    assets_urls = [[k,scene.assets[k].href] for k in scene.get_assets().keys() if k != 'index']\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    #loop run the assets\n",
    "    for k,url in assets_urls:\n",
    "        \n",
    "        path = f\"{directory}/{Path(url).name}\"\n",
    "        if os.path.exists(path):\n",
    "            continue\n",
    "        if verbose:\n",
    "            print('downloading from ',url)\n",
    "        cache_asset(url,path)\n",
    "        with open(\"../../data/landsat/task/workaround_history.txt\",'a') as f:\n",
    "            f.write(f\"{url}\\n\")\n",
    "    # save stac information if missing\n",
    "    if not os.path.exists(f\"{directory}/{scene.id}_stac.json\"):\n",
    "        with open(f\"{directory}/{scene.id}_stac.json\",\"w\") as w:\n",
    "            json.dump(stac_json,w)\n",
    "        with open(\"../../data/landsat/task/workaround_history.txt\",'a') as f:\n",
    "                f.write(f\"{usgs_item_url}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_pull_files(\"LC09_L1GT_011008_20221019_20221019_02_T2\",verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_products_l1 = pd.read_csv(\"../../../data/Landsat/missing_id.csv\").id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chronological_list = sorted(missing_products_l1, key=lambda x: x.split('_')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_files = ['LC09_L1TP_108078_20220520_20220520_02_T1',\n",
    " 'LC09_L1TP_108082_20220520_20220520_02_T1',\n",
    " 'LC09_L1TP_108083_20220520_20220520_02_T1',\n",
    " 'LC09_L1GT_108057_20220520_20220520_02_T2',\n",
    " 'LC09_L1TP_108233_20220520_20220520_02_T2',\n",
    " 'LC09_L1TP_108062_20220520_20220520_02_T1',\n",
    " 'LC09_L1TP_108070_20220520_20220520_02_T1',\n",
    " 'LC09_L1TP_200044_20221015_20221015_02_T1',\n",
    " 'LC09_L1TP_045016_20221017_20221018_02_T1',\n",
    " 'LC09_L1TP_045014_20221017_20221018_02_T1',\n",
    " 'LC09_L1TP_029047_20221017_20221017_02_T1',\n",
    " 'LC09_L1TP_029041_20221017_20221017_02_T1',\n",
    " 'LC09_L1TP_029035_20221017_20221017_02_T1',\n",
    " 'LC09_L1TP_029044_20221017_20221017_02_T1',\n",
    " 'LC09_L1TP_029033_20221017_20221017_02_T1',\n",
    " 'LC09_L1TP_045007_20221017_20221018_02_T1',\n",
    " 'LC09_L1TP_029046_20221017_20221017_02_T1',\n",
    " 'LC09_L1TP_029042_20221017_20221017_02_T1',\n",
    " 'LC09_L1TP_029043_20221017_20221017_02_T1',\n",
    " 'LC09_L1TP_029036_20221017_20221017_02_T1',\n",
    " 'LC09_L1TP_045008_20221017_20221018_02_T2',\n",
    " 'LC09_L1TP_029037_20221017_20221017_02_T1',\n",
    " 'LC09_L1TP_029031_20221017_20221017_02_T1',\n",
    " 'LC09_L1TP_029045_20221017_20221017_02_T1',\n",
    " 'LC09_L1TP_045015_20221017_20221018_02_T1',\n",
    " 'LC09_L1TP_029032_20221017_20221017_02_T1',\n",
    " 'LC09_L1TP_029034_20221017_20221017_02_T1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sid in tqdm.tqdm(missing_files):\n",
    "    hand_pull_files(sid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "41b0779e812f5c4dddd7de0cd8299bb9abad67a3737af553fcc301799477cf88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
